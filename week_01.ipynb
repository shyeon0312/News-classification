{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prerequisites__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vaiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from eunjeon import Mecab\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def get_scores(y_test, predicted):\n",
    "    print('-------------------------')\n",
    "    print('Accuracy_score = ', accuracy_score(y_test, predicted))\n",
    "    print('precision_score = ', precision_score(y_test, predicted))\n",
    "    print('recall_score = ', recall_score(y_test, predicted))\n",
    "    print('f1_score = ', f1_score(y_test, predicted))\n",
    "    print('-------------------------\\n')\n",
    "    \n",
    "    confusion_mat = confusion_matrix(y_test, predicted)\n",
    "    print(confusion_mat)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    # Get writer_score(ratio of tag_1)\n",
    "    def get_writer_score(self, data, train=False):\n",
    "        if train == True:\n",
    "            df_writer = data[['writerName', 'tag']].groupby('writerName', as_index=False).sum() # 신문사별 tag==1 개수 추출\n",
    "            df_writer.rename(columns={'tag':'tag_1'}, inplace=True)\n",
    "\n",
    "            total_cnt = data[['writerName', 'tag']].groupby(['writerName'], as_index=False).count()['tag'] # 신문사별 기사 개수 추출\n",
    "            df_writer['total_cnt'] = total_cnt\n",
    "\n",
    "            df_writer['writer_score'] = df_writer['tag_1']/df_writer['total_cnt'] # 신문사별 tag==1인 비율 계산\n",
    "            \n",
    "            df_writer.to_csv('../data/df_writer.csv', index=False)\n",
    "        else:\n",
    "            return\n",
    "\n",
    "\n",
    "    def merge_writer_score(self, data):\n",
    "        df_writer = pd.read_csv('../data/df_writer.csv')\n",
    "        data = pd.merge(data, df_writer[['writerName', 'writer_score']], how='left', on='writerName') # writerName 기준으로 merge\n",
    "        new_data = data[['date', 'writer_score', 'title', 'content', 'tag']]\n",
    "        return new_data\n",
    "    \n",
    "    def title_tf_idf(self, data, target):\n",
    "        # Mecab 객체 선언\n",
    "        mecab = Mecab()\n",
    "        \n",
    "        # Get nouns from df['title']\n",
    "        x_data = data['title'].apply(lambda x: ' '.join(mecab.nouns(x)))\n",
    "        \n",
    "        # Get vector count\n",
    "        # count_vect = CountVectorizer()\n",
    "        #X_counts = count_vect.fit_transform(x_data)\n",
    "        tfidf_vect = TfidfVectorizer()\n",
    "        X_counts = tfidf_vect.fit_transform(x_data)\n",
    "        # Save word vector\n",
    "        #pickle.dump(count_vect.vocabulary_, open(\"count_vector.pkl\",\"wb\"))\n",
    "\n",
    "        # Transform word vector to ti-idf\n",
    "        tfidf_transformer = TfidfTransformer()\n",
    "        X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "        df_tfidf = pd.DataFrame(X_tfidf.toarray())\n",
    "        df_tfidf['writer_score'] = data['writer_score']\n",
    "        df_tfidf[target] = data[target]\n",
    "        # df_tfidf['date'] = data['date']\n",
    "        df_tfidf.columns = list(map(str, list(df_tfidf.columns)))\n",
    "\n",
    "        # save tf-idf\n",
    "        #pickle.dump(tfidf_transformer, open(\"tfidf.pkl\",\"wb\"))\n",
    "        return df_tfidf\n",
    "\n",
    "    def data_transformation(self, data, target):\n",
    "        # transform NA to 0\n",
    "        data.fillna(0, inplace=True)\n",
    "        \n",
    "        # make date 0-1\n",
    "        # data['date'] = data['date']/30000000\n",
    "        \n",
    "        # create a feature matrix\n",
    "        X = data.drop(target, axis=1)\n",
    "\n",
    "        # create a target vector\n",
    "        y = data[target]\n",
    "        \n",
    "        # return the feature matrix and target vector\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write a function to perform data exploration\n",
    "# def perform_data_exploration(file_with_path):\n",
    "    \n",
    "#     # create an object of DataExploration class\n",
    "#     data_exploration = DataExploration()\n",
    "\n",
    "#     # load data HR_comma_sep.csv\n",
    "#     data = data_exploration.load_data(file_with_path)\n",
    "\n",
    "#     # Perform exploration\n",
    "#     data_exploration.data_exploration(data)\n",
    "    \n",
    "#     # explore data\n",
    "#     data_exploration.data_visualization(data)\n",
    "    \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to perform data preprocessing\n",
    "def perform_data_preprocessing(data, target, train=False):\n",
    "    # use DataPreprocessing class to perform data preprocessing\n",
    "    # create an object of DataPreprocessing class\n",
    "    data_preprocessing = DataPreprocessing()\n",
    "\n",
    "    data_preprocessing.get_writer_score(data, train)\n",
    "    new_data = data_preprocessing.merge_writer_score(data)\n",
    "    new_data = data_preprocessing.title_tf_idf(new_data, target)\n",
    "\n",
    "    # perform data_transformation\n",
    "    #data_preprocessing.title_transformation(data)\n",
    "    X, y = data_preprocessing.data_transformation(new_data, target)\n",
    "\n",
    "    return  X, y\n",
    "\n",
    "def data_splitting(X, y):\n",
    "    # split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=1004)\n",
    "\n",
    "    # return the training and testing sets\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAIN & TEST SET\n",
    "\n",
    "train = True  # train set으로만 결과 확인 시 True\n",
    "\n",
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "df = pd.concat([df_train, df_test])\n",
    "X, y = perform_data_preprocessing(df, 'tag', train=False)\n",
    "\n",
    "if train:\n",
    "    X_train, X_test, y_train, y_test = data_splitting(X[:8000], y[:8000])\n",
    "else:\n",
    "    X_train = X[:len(df_train)]; X_test = X[len(df_train):]\n",
    "    y_train = y[:len(df_train)]; y_test = y[len(df_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "-------------------------\n",
      "Accuracy_score =  0.91125\n",
      "precision_score =  0.8532763532763533\n",
      "recall_score =  0.9388714733542319\n",
      "f1_score =  0.8940298507462687\n",
      "-------------------------\n",
      "\n",
      "[[859 103]\n",
      " [ 39 599]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train logistic regression model\n",
    "classifier = LogisticRegression(random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "predicted_prob = classifier.predict_proba(X_test)\n",
    "\n",
    "if train :\n",
    "    result_nb = pd.DataFrame({'true_labels':y_test, 'predicted_labels':predicted})\n",
    "    print('Logistic Regression')\n",
    "    get_scores(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive_Bayes\n",
      "-------------------------\n",
      "Accuracy_score =  0.94\n",
      "precision_score =  0.9234375\n",
      "recall_score =  0.9263322884012539\n",
      "f1_score =  0.9248826291079814\n",
      "-------------------------\n",
      "\n",
      "[[913  49]\n",
      " [ 47 591]]\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "#pickle.dump(clf, open(\"svm.pkl\", \"wb\"))\n",
    "\n",
    "#SAVE MODEL\n",
    "#pickle.dump(clf, open(\"nb_model.pkl\", \"wb\"))\n",
    "\n",
    "predicted = clf.predict(X_test)\n",
    "predicted_prob = clf.predict_proba(X_test)\n",
    "\n",
    "if train:\n",
    "    result_nb = pd.DataFrame({'true_labels':y_test, 'predicted_labels':predicted})\n",
    "\n",
    "    print('Naive_Bayes')\n",
    "    get_scores(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax\n",
      "-------------------------\n",
      "Accuracy_score =  0.951875\n",
      "precision_score =  0.9560975609756097\n",
      "recall_score =  0.9216300940438872\n",
      "f1_score =  0.9385474860335195\n",
      "-------------------------\n",
      "\n",
      "[[935  27]\n",
      " [ 50 588]]\n",
      "SVM\n",
      "-------------------------\n",
      "Accuracy_score =  0.951875\n",
      "precision_score =  0.9560975609756097\n",
      "recall_score =  0.9216300940438872\n",
      "f1_score =  0.9385474860335195\n",
      "-------------------------\n",
      "\n",
      "[[935  27]\n",
      " [ 50 588]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_neural = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(128,), max_iter=10000, random_state=1)\n",
    "clf_neural.fit(X_train, y_train)\n",
    "pickle.dump(clf_neural, open(\"softmax.pkl\", \"wb\"))\n",
    "\n",
    "predicted = clf_neural.predict(X_test)\n",
    "predicted_prob = clf_neural.predict_proba(X_test)\n",
    "\n",
    "\n",
    "\n",
    "if train:\n",
    "    result_svm = pd.DataFrame({'true_labels':y_test, 'predicted_labels':predicted})\n",
    "\n",
    "    print('Softmax')\n",
    "    get_scores(y_test, predicted)\n",
    "else:\n",
    "    df_proba = pd.DataFrame(predicted_prob).rename(columns={0:'prob_0', 1:'prob_1'})\n",
    "    df_test['tag'] = predicted\n",
    "    df_test['prob_0'] = df_proba['prob_0']\n",
    "    df_test['prob_1'] = df_proba['prob_1']\n",
    "    \n",
    "    df_test.to_csv('../data/result_ksh_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "-------------------------\n",
      "Accuracy_score =  0.9475\n",
      "precision_score =  0.9196969696969697\n",
      "recall_score =  0.95141065830721\n",
      "f1_score =  0.9352850539291218\n",
      "-------------------------\n",
      "\n",
      "[[909  53]\n",
      " [ 31 607]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.LinearSVC()\n",
    "clf_svm.fit(X_train, y_train)\n",
    "\n",
    "#pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))\n",
    "\n",
    "predicted = clf_svm.predict(X_test)\n",
    "\n",
    "if train:\n",
    "    result_svm = pd.DataFrame({'true_labels':y_test, 'predicted_labels':predicted})\n",
    "\n",
    "    print('SVM')\n",
    "    get_scores(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "-------------------------\n",
      "Accuracy_score =  0.94875\n",
      "precision_score =  0.9276923076923077\n",
      "recall_score =  0.945141065830721\n",
      "f1_score =  0.9363354037267081\n",
      "-------------------------\n",
      "\n",
      "[[915  47]\n",
      " [ 35 603]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "clf_svm = svm.LinearSVC()\n",
    "clf = CalibratedClassifierCV(clf_svm, method='sigmoid') \n",
    "clf.fit(X_train, y_train)\n",
    "y_proba = clf.predict_proba(X_test)\n",
    "\n",
    "#pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))\n",
    "\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "if train:\n",
    "    result_svm = pd.DataFrame({'true_labels':y_test, 'predicted_labels':predicted})\n",
    "\n",
    "    print('SVM')\n",
    "    get_scores(y_test, predicted)\n",
    "else:\n",
    "    df_proba = pd.DataFrame(y_proba).rename(columns={0:'prob_0', 1:'prob_1'})\n",
    "    df_test['tag'] = predicted\n",
    "    df_test['prob_0'] = df_proba['prob_0']\n",
    "    df_test['prob_1'] = df_proba['prob_1']\n",
    "    \n",
    "    # df_test.to_csv('../data/result_ksh_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "\n",
    "# clf_svm = svm.SVC(kernel='linear', probability=True)\n",
    "# clf_svm.fit(X_train, y_train)\n",
    "\n",
    "# #pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))\n",
    "\n",
    "# predicted = clf_svm.predict(X_test)\n",
    "# predicted_prob = clf_svm.predict_proba(X_test)\n",
    "# result_svm = pd.DataFrame({'true_labels':y_test, 'predicted_labels':predicted})\n",
    "\n",
    "# print('SVM')\n",
    "# get_scores(y_test, predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from keras.utils import to_categorical\n",
    "import sys\n",
    "import multiprocessing\n",
    "import re\n",
    "\n",
    "# write a function to perform data preprocessing\n",
    "def perform_data_preprocessing_for_doc2vec(data, target, train=False):\n",
    "    # use DataPreprocessing class to perform data preprocessing\n",
    "    # create an object of DataPreprocessing class\n",
    "    data_preprocessing = DataPreprocessing()\n",
    "\n",
    "    data_preprocessing.get_writer_score(data, train)\n",
    "    new_data = data_preprocessing.merge_writer_score(data)\n",
    "    new_data['content'].fillna('', inplace=True)\n",
    "\n",
    "    return  new_data\n",
    "\n",
    "def text_preprocessing(text_list):\n",
    "    \n",
    "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] #불용어 설정\n",
    "    tokenizer = Mecab() #형태소 분석기 \n",
    "    token_list = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        txt = re.sub('[^가-힣a-z]', ' ', text) #한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
    "        token = tokenizer.nouns(txt) #형태소 분석\n",
    "        token = [t for t in token if t not in stopwords or type(t) != float] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
    "        #token_list.append(' '.join(token))\n",
    "        token_list.append(token)\n",
    "        \n",
    "    return token_list, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "df = perform_data_preprocessing_for_doc2vec(df, target='tag', train=True)\n",
    "df['content'] = df[['title', 'content']].apply(lambda x: str(x['title'] + '. ' + x['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>writer_score</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원</td>\n",
       "      <td>정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원. 정부가 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230215</td>\n",
       "      <td>0.021552</td>\n",
       "      <td>인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각</td>\n",
       "      <td>인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각. 기사내용 요약 법원 \"피의 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230214</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>튀르키예 강진에 우리나라 지하수가 출렁였다</td>\n",
       "      <td>튀르키예 강진에 우리나라 지하수가 출렁였다. 튀르키예에서 발생한 강진에 우리나라의 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230215</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행</td>\n",
       "      <td>멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행. [데일리안 = 박영민 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230111</td>\n",
       "      <td>0.699872</td>\n",
       "      <td>美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\"</td>\n",
       "      <td>美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\". 국방부 대변인 \"실수...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>20230104</td>\n",
       "      <td>0.544474</td>\n",
       "      <td>외교부, 오는 12일 日강제징용 해법 토론회 연다</td>\n",
       "      <td>외교부, 오는 12일 日강제징용 해법 토론회 연다. 국회서 한일의원연맹과 공동 개최...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>20230215</td>\n",
       "      <td>0.667622</td>\n",
       "      <td>교원단체, '유치원' 명칭은 일제 잔재…'유아학교'로 변경해야</td>\n",
       "      <td>교원단체, '유치원' 명칭은 일제 잔재…'유아학교'로 변경해야. 교육부가 2025년...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>20230212</td>\n",
       "      <td>0.699872</td>\n",
       "      <td>내일 한일외교차관회담… '강제동원 해법' 이견 좁힐까</td>\n",
       "      <td>내일 한일외교차관회담… '강제동원 해법' 이견 좁힐까. 13일 워싱턴 한미일 차관협...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>20230214</td>\n",
       "      <td>0.029167</td>\n",
       "      <td>최상호 국립오페라단 단장</td>\n",
       "      <td>최상호 국립오페라단 단장. 문화체육관광부가 재단법인 국립오페라단 단장 겸 예술감독에...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>20230214</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>이통통신 3사, 갤럭시 S23 시리즈 사전 개통 시작…'구매 꿀팁은?'</td>\n",
       "      <td>이통통신 3사, 갤럭시 S23 시리즈 사전 개통 시작…'구매 꿀팁은?'. [스포츠서...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  writer_score                                     title  \\\n",
       "0     20230214      0.000000  정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원   \n",
       "1     20230215      0.021552             인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각   \n",
       "2     20230214      0.151515                   튀르키예 강진에 우리나라 지하수가 출렁였다   \n",
       "3     20230215      0.010638          멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행   \n",
       "4     20230111      0.699872         美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\"   \n",
       "...        ...           ...                                       ...   \n",
       "7995  20230104      0.544474               외교부, 오는 12일 日강제징용 해법 토론회 연다   \n",
       "7996  20230215      0.667622        교원단체, '유치원' 명칭은 일제 잔재…'유아학교'로 변경해야   \n",
       "7997  20230212      0.699872             내일 한일외교차관회담… '강제동원 해법' 이견 좁힐까   \n",
       "7998  20230214      0.029167                             최상호 국립오페라단 단장   \n",
       "7999  20230214      0.125000   이통통신 3사, 갤럭시 S23 시리즈 사전 개통 시작…'구매 꿀팁은?'   \n",
       "\n",
       "                                                content  tag  \n",
       "0     정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원. 정부가 ...    0  \n",
       "1     인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각. 기사내용 요약 법원 \"피의 ...    0  \n",
       "2     튀르키예 강진에 우리나라 지하수가 출렁였다. 튀르키예에서 발생한 강진에 우리나라의 ...    0  \n",
       "3     멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행. [데일리안 = 박영민 ...    0  \n",
       "4     美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\". 국방부 대변인 \"실수...    1  \n",
       "...                                                 ...  ...  \n",
       "7995  외교부, 오는 12일 日강제징용 해법 토론회 연다. 국회서 한일의원연맹과 공동 개최...    1  \n",
       "7996  교원단체, '유치원' 명칭은 일제 잔재…'유아학교'로 변경해야. 교육부가 2025년...    0  \n",
       "7997  내일 한일외교차관회담… '강제동원 해법' 이견 좁힐까. 13일 워싱턴 한미일 차관협...    1  \n",
       "7998  최상호 국립오페라단 단장. 문화체육관광부가 재단법인 국립오페라단 단장 겸 예술감독에...    0  \n",
       "7999  이통통신 3사, 갤럭시 S23 시리즈 사전 개통 시작…'구매 꿀팁은?'. [스포츠서...    0  \n",
       "\n",
       "[8000 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#doc2vec parameters\n",
    "vector_size = 1000\n",
    "window_size = 15\n",
    "word_min_count = 2\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 1 #0 = dbow; 1 = dmpv\n",
    "worker_count = cores #number of parallel processes\n",
    "\n",
    "tokened_content, mecab = text_preprocessing(df['content'])\n",
    "onehot_y = pd.DataFrame(to_categorical(df['tag']))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tokened_content, onehot_y, test_size=0.2, random_state=1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TaggedDocuments\n",
    "train_docs = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(X_train)]\n",
    "test_docs = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaiv\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Train Doc2Vec model\n",
    "model = Doc2Vec(min_count=word_min_count, size=vector_size, alpha=0.025, min_alpha=0.025, seed=1234, workers=worker_count)\n",
    "model.build_vocab(train_docs)\n",
    "model.train(train_docs, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer vectors for training and testing sets\n",
    "train_vectors = [model.infer_vector(doc.words) for doc in train_docs]\n",
    "test_vectors = [model.infer_vector(doc.words) for doc in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_vectors)\n",
    "df_train.index = y_train.index\n",
    "df_train.index\n",
    "\n",
    "df_train = df_train.join(df['writer_score'], how='left')\n",
    "df_train.columns = list(map(str, list(df_train.columns)))\n",
    "\n",
    "df_test = pd.DataFrame(test_vectors)\n",
    "df_test.index = y_test.index\n",
    "df_test.index\n",
    "\n",
    "df_test = df_test.join(df['writer_score'], how='left')\n",
    "df_test.columns = list(map(str, list(df_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_train = df[['tag']].loc[y_train.index]['tag']\n",
    "df_y_test = df[['tag']].loc[y_test.index]['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax\n",
      "-------------------------\n",
      "Accuracy_score =  0.93375\n",
      "precision_score =  0.9130434782608695\n",
      "recall_score =  0.9216300940438872\n",
      "f1_score =  0.9173166926677067\n",
      "-------------------------\n",
      "\n",
      "[[906  56]\n",
      " [ 50 588]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_neural = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(128,), max_iter=10000, random_state=1)\n",
    "clf_neural.fit(df_train, df_y_train)\n",
    "pickle.dump(clf_neural, open(\"softmax.pkl\", \"wb\"))\n",
    "\n",
    "predicted = clf_neural.predict(df_test)\n",
    "predicted_prob = clf_neural.predict_proba(df_test)\n",
    "result_softmax = pd.DataFrame({'true_labels':df_y_test, 'predicted_labels':predicted})\n",
    "\n",
    "print('Softmax')\n",
    "get_scores(df_y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "-------------------------\n",
      "Accuracy_score =  0.921875\n",
      "precision_score =  0.8952234206471494\n",
      "recall_score =  0.9106583072100314\n",
      "f1_score =  0.9028749028749028\n",
      "-------------------------\n",
      "\n",
      "[[894  68]\n",
      " [ 57 581]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaiv\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "clf_svm = svm.LinearSVC()\n",
    "clf_svm.fit(df_train, df_y_train)\n",
    "\n",
    "#pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))\n",
    "\n",
    "predicted = clf_svm.predict(df_test)\n",
    "result_svm = pd.DataFrame({'true_labels':df_y_test, 'predicted_labels':predicted})\n",
    "\n",
    "print('SVM')\n",
    "get_scores(df_y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in range(len(tokened_content)):             # tokened_X의 길이만큼 돌면서 제일 긴문장의 길이 알아내야 => max_20, wordsize_11919\n",
    "    if max_len < len(tokened_content[i]):\n",
    "        max_len = len(tokened_content[i])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 1001) (6400, 2)\n",
      "(1600, 1001) (1600, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, y_train.shape)\n",
    "print(df_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 1001) (6400, 2)\n",
      "(1600, 1001) (1600, 2)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1113, 300)         3000000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1113, 32)          48032     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 1113, 32)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1113, 128)         82432     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1113, 128)         0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 1113, 128)         131584    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1113, 128)         0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 1113, 64)          49408     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1113, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 71232)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               9117824   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,429,538\n",
      "Trainable params: 12,429,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = np.load(                                 # .npy 파일 로드\n",
    "#    '../model/news_classification_ksh-main/news_classification_ksh-main/models/news_data_max_26_wordsize_12256.npy', allow_pickle=True)\n",
    "\n",
    "print(df_train.shape, y_train.shape)\n",
    "print(df_test.shape, y_test.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 300, input_length=1113))                           # 11919 => 300으로 줄여주는 # 본인단어 개수로 (난 11919)\n",
    "\n",
    "# 임베딩 레이어의 역할 : 수치적으로 계산하여 안되는 명목척도\n",
    "# 의미공간상의 배치, 11919차원 => 300차원으로 줄일 것. => 각각의 형태소가 의미를 계산할 수 있게됨\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=5, padding='same', activation='relu'))     # Relu\n",
    "model.add(MaxPool1D(pool_size=1))                                           # 1써주면 달라지는 거 없지만 그래도 습관적으로\n",
    "#model.add(GRU(128, activation='tanh', return_sequences=True))               # 리턴시퀀스는 나중에 설명\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))                                                     # 과적합 막기위해\n",
    "#model.add(GRU(64, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True))                                       # GRU는 여기까지\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))                                    # 좀 딥러닝같다\n",
    "model.add(Dense(2, activation='softmax'))                                   # 카테고리 여섯개. 다중카테고리.\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',            # 옵티마이저 adam\n",
    "              metrics=['accuracy'])\n",
    "# fit_hist = model.fit(df_train, y_train, batch_size=128,\n",
    "#                      epochs=10, validation_data=(df_test, y_test))\n",
    "# model.save('./models/news_category_classfication_model_{}.h5'.format(       # 모델저장하기. (한참 돌렸는데 저장안하면 말짱꽝, 중간에 팅겨도!)\n",
    "#     np.round(fit_hist.history['val_accuracy'][-1], 3)))                     # 소수점 아래 3째까지 val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e650a76af8b61bac68830ccd08220010f4794930f80523975eef46125932fe32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
