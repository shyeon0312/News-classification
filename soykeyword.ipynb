{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction using 'soykeyword'\n",
    "\n",
    "### 설명\n",
    "- 한 문서 집합의 키워드 :\n",
    "    - 다른 문서 집합과 해당 문서 집합을 구분할 수 있는 질 좋은 단어(구분력, discriminative power)\n",
    "    - 해당 집합을 잘 설명할 수 있는 (설명력, high coverage) 단어\n",
    "    - 빈도수가 낮은 단어는 한 집합에서만 등장할 가능성이 높기 때문에 구분력은 크지만 설명력이 약합니다\n",
    "    - 제안된 두 가지 알고리즘은 높은 설명력과 구분력을 동시에 지니는 단어들을 키워드로 선택합니다.\n",
    "<br><br>\n",
    "- 연관어 :\n",
    "    - 기준 단어가 포함된 문서 집합과 포함되지 않은 문서 집합을 구분하는 키워드를 연관어로 정의\n",
    "    - co-occurrence 가 높은 단어라는 의미이기도 합니다\n",
    "    - co-occurrence 가 높으면서도 설명력이 좋은 단어를 선택합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in c:\\users\\vaiv\\anaconda3\\lib\\site-packages (5.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soykeyword in c:\\users\\vaiv\\anaconda3\\lib\\site-packages (0.0.14)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\vaiv\\anaconda3\\lib\\site-packages (from soykeyword) (1.21.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\vaiv\\anaconda3\\lib\\site-packages (from soykeyword) (1.0.2)\n",
      "Requirement already satisfied: psutil>=5.0.1 in c:\\users\\vaiv\\anaconda3\\lib\\site-packages (from soykeyword) (5.9.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\vaiv\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->soykeyword) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vaiv\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->soykeyword) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\vaiv\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->soykeyword) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\vaiv\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil\n",
    "!pip install soykeyword"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version  3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]\n",
      "numpy version  1.21.0\n",
      "sklearn version  1.0.2\n",
      "psutil version  5.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import psutil\n",
    "\n",
    "print('python version ', sys.version) # >=3.4\n",
    "print('numpy version ', np.__version__) # >= 1.12.1\n",
    "print('sklearn version ', sklearn.__version__) # >= 0.18\n",
    "print(\"psutil version \", psutil.__version__) # >= 5.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from eunjeon import Mecab\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Keyword Extractor\n",
    "- sparse matrix x를 extractor에 입력\n",
    "- index2word : word idx에 대한 단어 list 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soykeyword.lasso import LassoKeywordExtractor\n",
    "\n",
    "lassobased_extractor = LassoKeywordExtractor(min_tf=20, min_df=10)\n",
    "lassobased_extractor.train(x, index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = lassobased_extractor.extract_from_docs(\n",
    "    documents, \n",
    "    min_num_of_keywords=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassobased_extractor.extract_from_word(\n",
    "    '아이오아이',\n",
    "    min_num_of_keywords=30\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction using Proportion Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docID</th>\n",
       "      <th>date</th>\n",
       "      <th>writerName</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20230214</td>\n",
       "      <td>머니S</td>\n",
       "      <td>정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원</td>\n",
       "      <td>정부가 미국 AI 개발업체인 '오픈AI'(OpenAI)의 '챗GPT'(ChatGPT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20230215</td>\n",
       "      <td>뉴시스</td>\n",
       "      <td>인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각</td>\n",
       "      <td>기사내용 요약 법원 \"피의 사실 일부 다툼 여지, 불구속 상태 방어권 보장 필요\" ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20230214</td>\n",
       "      <td>아이뉴스24</td>\n",
       "      <td>튀르키예 강진에 우리나라 지하수가 출렁였다</td>\n",
       "      <td>튀르키예에서 발생한 강진에 우리나라의 지하수가 출렁였다는 관측 보고가 나왔다. 한국...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20230215</td>\n",
       "      <td>데일리안</td>\n",
       "      <td>멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행</td>\n",
       "      <td>[데일리안 = 박영민 기자] 오픈마켓 멸치쇼핑이 2023년 신입 및 경력 사원을 대...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20230111</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\"</td>\n",
       "      <td>국방부 대변인 \"실수 바로잡기 위해 내무부와 협력\"…'오류 발견' 가족 등에 연락 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   docID      date writerName                                     title  \\\n",
       "0      1  20230214        머니S  정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원   \n",
       "1      2  20230215        뉴시스             인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각   \n",
       "2      3  20230214     아이뉴스24                   튀르키예 강진에 우리나라 지하수가 출렁였다   \n",
       "3      4  20230215       데일리안          멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행   \n",
       "4      5  20230111        뉴스1         美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\"   \n",
       "\n",
       "                                             content  tag  \n",
       "0  정부가 미국 AI 개발업체인 '오픈AI'(OpenAI)의 '챗GPT'(ChatGPT...    0  \n",
       "1  기사내용 요약 법원 \"피의 사실 일부 다툼 여지, 불구속 상태 방어권 보장 필요\" ...    0  \n",
       "2  튀르키예에서 발생한 강진에 우리나라의 지하수가 출렁였다는 관측 보고가 나왔다. 한국...    0  \n",
       "3  [데일리안 = 박영민 기자] 오픈마켓 멸치쇼핑이 2023년 신입 및 경력 사원을 대...    0  \n",
       "4  국방부 대변인 \"실수 바로잡기 위해 내무부와 협력\"…'오류 발견' 가족 등에 연락 ...    1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion ratio를 이용하는 키워드 추출 방법은 CorpusbasedKeywordExtractor와 MatrixbasedKeywordExtractor 두 종류로 구현되어 있습니다. CorpusbasedKeywordExtractor는 아래 예시처럼 text 파일에서 키워드를 직접 추출하는 코드이며, MatrixbasedKeywordExtractor는 sparse format으로 만들어둔 term frequency matrix에서 키워드를 추출하는 코드입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proportion ratio의 개념은 아래와 같습니다.\n",
    "\n",
    "키워드란 사실 명확한 정의가 있는 단어가 아닙니다. 흔히 사용하는 키워드의 정의에는 자주 나오는 단어나, TF-IDF 값이 있습니다. 이들은 각자의 관점으로 키워드를 정의한 것입니다. 자주 나오는 단어를 키워드로 정의하는 것은 많이 나올수록 키워드라는 의미이며, 이 때에는 조사와 같은 단어가 키워드가 될 수도 있습니다. 이를 보완하기 위해 품사 판별 (Part-of-Speech tagging)을 한 뒤, 명사만을 추출하여 최빈어를 키워드로 선정하는 것은 합리적이라 생각됩니다. TF-IDF를 키워드로 사용하는 방법은 조금 위험한 방법입니다. IDF는 단어가 등장한 문서의 개수가 적을수록 커지기 때문에 오히려 노이즈일 가능성이 높기 때문입니다.\n",
    "\n",
    "그 외에도 chi-square를 이용하는 방법도 있습니다. 제가 제안하는 Proportion ratio는 이 방법에 가깝습니다. 기본 컨셉은 아래와 같습니다. '뉴스에서의 키워드'를 선택하라는 말은 애매모호합니다. 하지만, '오늘의 뉴스에서의 키워드'나 '아이오아이에 대한 문서에서의 키워드'라는 말은 조금 더 명료합니다. 키워드에 대한 관점이 생기기 때문입니다. 좀 더 자세히, 여름철 뉴스에서는 '호우'라는 단어가 0.1% 씩 늘 등장한다고 가정합시다. 어느날 '호우'라는 단어가 평상시와 다르게 0.9% 등장하였다면 (평상시보다 9배), 이 날은 정말로 호우가 내려서 뉴스에 그 단어가 자주 등장했을 가능성이 높습니다. 그렇다면 '호우'는 그날의 키워드가 될 수 있을 것입니다. 이를 수치로 만들기 위해서 다음과 같은 지표를 만들었습니다.\n",
    "\n",
    "score(w) = P(w|Dt) / { P(w|Dt) + P(w|Dr) }\n",
    "\n",
    "P(w|Dt): target document에서 단어 w가 출현한 비율\n",
    "P(w|Dr): reference document에서 단어 w가 출현한 비율"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target document란 키워드를 정의하고 싶은 문서 집합을 의미합니다. '아이오아이라는 단어가 포함된 뉴스'라던가, 어느날의 뉴스가 됩니다. Reference document는 평상시의 문서 집합입니다. '아이오아이'를 포함한 연예뉴스 가 될 수도, 하루치 전체 뉴스가 될 수도 있습니다. 어느 하루의 뉴스의 키워드를 선택하기 위해서는 이전 10일치의 뉴스를 reference document로 선택할 수 있습니다.\n",
    "\n",
    "이렇게 keyword score를 정의하면 score(w)는 [0, 1] 사이의 값이 됩니다. 평상시 호우가 0.1% 등장하였다가 오늘 0.9% 등장하였다면 score는 0.9 / (0.1 + 0.9) = 0.9 입니다. 평상시와 같이 0.1% 등장하였다면 (0.1 / (0.1 + 0.1)) = 0.5가 됩니다. 0.5란 평상시와 다르지 않다는 의미이며, 그 이하는 평상시보다 등장하지 않았다는 의미입니다. 하지만 0.5보다 작은 값은 의미가 없습니다. target document set은 reference document set보다 훨씬 작은 집합이기 때문에 많은 단어를 포함하지 않을 수 있기 때문입니다. 대신 0.5보다 큰 score를 지니는 단어들은 평상시보다 자주 등장한 단어임을 의미합니다. 이때에는 한가지 false alarm이 생길 수 있습니다. 애초에 자주 등장하지 않는 단어이기 때문에 target documents에만 등장하는 단어는 1.0에 가까운 score를 가지게 됩니다. 이를 방지하기 위해서 최소한 등장해야 하는 단어 빈도수를 한정할 필요가 있습니다. 그래서 키워드를 선택할 때 항상 parameter로 min_frequency를 넣도록 하였습니다.\n",
    "\n",
    "keywords = corpusbased_extractor.extract_from_word('아이오아이', min_score=0.8, min_frequency=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docID</th>\n",
       "      <th>date</th>\n",
       "      <th>writerName</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>133</td>\n",
       "      <td>20230105</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>[속보]軍 \"북한 무인기, 용산 비행금지구역 북쪽 끝 일부 지나가\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>179</td>\n",
       "      <td>20230104</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>[속보]軍 \"합동 드론사령부 조기 창설… 드론킬러 드론 신속 개발\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>217</td>\n",
       "      <td>20230212</td>\n",
       "      <td>코리아중앙데일리</td>\n",
       "      <td>Greek epics</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>386</td>\n",
       "      <td>20230214</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>[속보] 한미일 \"北과 대화에 열려 있어…비핵화 대화 복귀 촉구\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>20230203</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>[속보] 美 F-22·F-35B, 韓 F-35A와 서해상에서 또 연합공중훈련</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7787</th>\n",
       "      <td>7788</td>\n",
       "      <td>20230214</td>\n",
       "      <td>중앙일보</td>\n",
       "      <td>[오늘의 날씨] 2월 14일</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7836</th>\n",
       "      <td>7837</td>\n",
       "      <td>20230113</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>[속보] 한일 외교장관 통화…강제징용 문제 의견교환</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7854</th>\n",
       "      <td>7855</td>\n",
       "      <td>20230209</td>\n",
       "      <td>아시아경제</td>\n",
       "      <td>[속보]北 \"전술핵운용부대 열병식 등장\"…김주애 참석</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7959</th>\n",
       "      <td>7960</td>\n",
       "      <td>20230131</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>[속보] 美 국방장관 \"앞으로 F-22·F-35·항모 전개 많이 할것\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7961</th>\n",
       "      <td>7962</td>\n",
       "      <td>20230215</td>\n",
       "      <td>코리아중앙데일리</td>\n",
       "      <td>Graduation season</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      docID      date writerName                                       title  \\\n",
       "132     133  20230105        뉴스1       [속보]軍 \"북한 무인기, 용산 비행금지구역 북쪽 끝 일부 지나가\"   \n",
       "178     179  20230104        뉴스1       [속보]軍 \"합동 드론사령부 조기 창설… 드론킬러 드론 신속 개발\"   \n",
       "216     217  20230212   코리아중앙데일리                                 Greek epics   \n",
       "385     386  20230214        뉴스1        [속보] 한미일 \"北과 대화에 열려 있어…비핵화 대화 복귀 촉구\"   \n",
       "399     400  20230203       연합뉴스  [속보] 美 F-22·F-35B, 韓 F-35A와 서해상에서 또 연합공중훈련   \n",
       "...     ...       ...        ...                                         ...   \n",
       "7787   7788  20230214       중앙일보                             [오늘의 날씨] 2월 14일   \n",
       "7836   7837  20230113       연합뉴스                [속보] 한일 외교장관 통화…강제징용 문제 의견교환   \n",
       "7854   7855  20230209      아시아경제               [속보]北 \"전술핵운용부대 열병식 등장\"…김주애 참석   \n",
       "7959   7960  20230131       연합뉴스     [속보] 美 국방장관 \"앞으로 F-22·F-35·항모 전개 많이 할것\"   \n",
       "7961   7962  20230215   코리아중앙데일리                           Graduation season   \n",
       "\n",
       "     content  tag  \n",
       "132      NaN    1  \n",
       "178      NaN    1  \n",
       "216      NaN    1  \n",
       "385      NaN    1  \n",
       "399      NaN    1  \n",
       "...      ...  ...  \n",
       "7787     NaN    0  \n",
       "7836     NaN    1  \n",
       "7854     NaN    1  \n",
       "7959     NaN    1  \n",
       "7961     NaN    0  \n",
       "\n",
       "[102 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['content'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaiv\\AppData\\Local\\Temp\\ipykernel_7660\\2103349724.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dropna['title_content'] = df_dropna['title'] + ' ' + df_dropna['content'] # title + content\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원 정부가 미...\n",
       "1    인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각 기사내용 요약 법원 \"피의 사...\n",
       "2    튀르키예 강진에 우리나라 지하수가 출렁였다 튀르키예에서 발생한 강진에 우리나라의 지...\n",
       "3    멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행 [데일리안 = 박영민 기...\n",
       "4    美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\" 국방부 대변인 \"실수 ...\n",
       "Name: title_content, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dropna = df.dropna(axis=0, subset=['content']) # content가 없는 기사는 drop --> 총 102개\n",
    "df_dropna['title_content'] = df_dropna['title'] + ' ' + df_dropna['content'] # title + content\n",
    "df_dropna['title_content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docID</th>\n",
       "      <th>date</th>\n",
       "      <th>writerName</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "      <th>title_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [docID, date, writerName, title, content, tag, title_content]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dropna[df_dropna['title_content'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사 추출\n",
    "mecab = Mecab()\n",
    "\n",
    "# Get nouns from df['title_content']\n",
    "tokenized_corpus_fname = df_dropna['title_content'].apply(lambda x: ' '.join(mecab.nouns(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       정부 반도체 석 박사 집중 육성 대학당 년 간 억 원 지원 정부 미국 개발 업체 오...\n",
       "1       인사 청탁 대가 금품 수수 의혹 전 소방 청장 영장 기각 기사 내용 요약 법원 피 ...\n",
       "2       튀르 키 예 강진 우리 나라 지하수 튀르 키 예 발생 강진 우리 나라 지하수 관측 ...\n",
       "3       멸치 쇼핑 년 신입 경력 사원 규모 공채 진행 데일리안 박영민 기자 오픈 마켓 멸치...\n",
       "4       美 국방부 추모 벽 전사자 명단 오류 유감 실수 국방부 대변인 실수 내무부 협력 오...\n",
       "                              ...                        \n",
       "7995    외교부 일 日 강제 징용 해법 토론회 국회 일 의원 연맹 공동 개최 데일리 권오석 ...\n",
       "7996    교원 단체 유치원 명칭 일제 잔재 유아 학교 변경 교육부 년 유치원 어린이집 하나 ...\n",
       "7997    일 외교 차관 회담 강제 동원 해법 이견 일 워싱턴 미일 차관 협의회 계기 양자 회...\n",
       "7998    최상호 국립오페라단 단장 문화 체육관 광부 재단법인 국립오페라단 단장 겸 예술 감독...\n",
       "7999    이통 통신 사 갤럭시 시리즈 사전 개통 시작 구매 팁 스포츠서울 황철 훈 기자 통신...\n",
       "Name: title_content, Length: 7898, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정부',\n",
       " '반도체',\n",
       " '석',\n",
       " '박사',\n",
       " '집중',\n",
       " '육성',\n",
       " '대학당',\n",
       " '년',\n",
       " '간',\n",
       " '억',\n",
       " '원',\n",
       " '지원',\n",
       " '정부',\n",
       " '미국',\n",
       " '개발',\n",
       " '업체',\n",
       " '오픈',\n",
       " '챗',\n",
       " '등장',\n",
       " '산업',\n",
       " '시장',\n",
       " '격변',\n",
       " '반도체',\n",
       " '산업',\n",
       " '고급',\n",
       " '재양',\n",
       " '대학원',\n",
       " '사업',\n",
       " '공고',\n",
       " '시행',\n",
       " '과학',\n",
       " '기술',\n",
       " '정보',\n",
       " '통신부',\n",
       " '다음',\n",
       " '달',\n",
       " '일',\n",
       " '세계',\n",
       " '전문',\n",
       " '인력',\n",
       " '양성',\n",
       " '대학원',\n",
       " '신설',\n",
       " '규모',\n",
       " '예산',\n",
       " '지원',\n",
       " '일',\n",
       " '정부',\n",
       " '개',\n",
       " '대학원',\n",
       " '선정',\n",
       " '년',\n",
       " '년',\n",
       " '동안',\n",
       " '대학당',\n",
       " '억',\n",
       " '원',\n",
       " '상당',\n",
       " '지원',\n",
       " '방침',\n",
       " '정부',\n",
       " '이번',\n",
       " '사업',\n",
       " '국가',\n",
       " '전략',\n",
       " '기술',\n",
       " '경제',\n",
       " '안보',\n",
       " '핵심',\n",
       " '품목',\n",
       " '반도체',\n",
       " '분야',\n",
       " '기술',\n",
       " '경쟁력',\n",
       " '미래',\n",
       " '유망',\n",
       " '시장',\n",
       " '창출',\n",
       " '세계',\n",
       " '수준',\n",
       " '반도체',\n",
       " '설계',\n",
       " '소프트웨어',\n",
       " '전문',\n",
       " '고급',\n",
       " '인재',\n",
       " '명',\n",
       " '양성',\n",
       " '목표',\n",
       " '신입',\n",
       " '교육',\n",
       " '가을',\n",
       " '학기',\n",
       " '시작',\n",
       " '대학',\n",
       " '정부',\n",
       " '지원',\n",
       " '반도체',\n",
       " '설계',\n",
       " '소프트웨어',\n",
       " '역량',\n",
       " '확보',\n",
       " '가능',\n",
       " '특화',\n",
       " '교육',\n",
       " '과정',\n",
       " '구성',\n",
       " '시스템',\n",
       " '소프트웨어',\n",
       " '관련',\n",
       " '전문가',\n",
       " '등',\n",
       " '우수',\n",
       " '연구',\n",
       " '진',\n",
       " '확보',\n",
       " '전임',\n",
       " '교원',\n",
       " '인',\n",
       " '이상',\n",
       " '반도체',\n",
       " '제작',\n",
       " '경험',\n",
       " '등',\n",
       " '실전',\n",
       " '역량',\n",
       " '재고',\n",
       " '기업',\n",
       " '참여',\n",
       " '프로젝트',\n",
       " '등',\n",
       " '교과목',\n",
       " '구성',\n",
       " '이',\n",
       " '연계',\n",
       " '기업',\n",
       " '인턴',\n",
       " '십',\n",
       " '팹',\n",
       " '리스',\n",
       " '설계',\n",
       " '개발',\n",
       " '전문',\n",
       " '학생',\n",
       " '창업',\n",
       " '지원',\n",
       " '등',\n",
       " '산학',\n",
       " '협력',\n",
       " '강화',\n",
       " '요구',\n",
       " '전영수',\n",
       " '과기부',\n",
       " '정보',\n",
       " '통신',\n",
       " '산업',\n",
       " '정책',\n",
       " '관은',\n",
       " '데이터',\n",
       " '센터',\n",
       " '자율',\n",
       " '주행',\n",
       " '사물',\n",
       " '인터넷',\n",
       " '등',\n",
       " '산업',\n",
       " '확산',\n",
       " '분야',\n",
       " '반도체',\n",
       " '활용',\n",
       " '확대',\n",
       " '반도체',\n",
       " '대학원',\n",
       " '등',\n",
       " '디지털',\n",
       " '첨단',\n",
       " '분야',\n",
       " '재양',\n",
       " '미래',\n",
       " '유망',\n",
       " '분야',\n",
       " '산업',\n",
       " '생태',\n",
       " '조성',\n",
       " '국가',\n",
       " '기술',\n",
       " '주권',\n",
       " '확보',\n",
       " '총력',\n",
       " '말']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus_fname[0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, df):\n",
    "        self.title = df['title']\n",
    "        self.content = df['content']\n",
    "        self.title_content = df['title_content']\n",
    "        self.length = 0\n",
    "    def __len__(self):\n",
    "        if self.length == 0:\n",
    "            self.length = len(df)\n",
    "        return self.length\n",
    "    def __iter__(self):\n",
    "        for text in self.title_content:\n",
    "            yield text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc=6, num words=35, 이라크 무역부 장관과 면담하는 원희룡 장관 (서울=연합뉴스) 원희룡 국토교통부 장관이 지난 25일 이라크 바그다드를 방문, 아티르 알 그레이리(Atheer Dawoud Salman Al Ghrairy) 이라크 무역부 장관과 면담하고 있다. 2023.1.27 [국토교통부 제공. 재판매 및 DB 금지] photo@yna.co.kr\n",
      "\n",
      "doc=7, num words=117, 내달 ‘윤 2월’ 서울시립 화장터 2배로 운영 ‘개장 화장’ 22일부터 예약 서울시설공단은 3년 만의 윤달을 앞두고 서울시립 장사시설의 하루 운영 화구 수를 2배로 확대한다고 15일 밝혔다. 오는 3월22일부터 4월19일까지는 3년 만에 돌아오는 윤달이다. ‘손 없는 달’이라고도 불리는 이 기간에는 개장유골 화장 예약 건수가 평소보다 많이 늘어나는 경향이 \n",
      "\n",
      "doc=8, num words=82, BTS 지민, 튀르키예·시리아 지진 피해 어린이 긴급구호 동참 [파이낸셜뉴스] 그룹 방탄소년단(BTS)의 지민이 ‘튀르키예·시리아 지진 피해 어린이 긴급구호’에 동참했다. 15일 유니세프한국위원회에 따르면 이날 방탄소년단의 지민이 ‘튀르키예·시리아 지진 피해 어린이 긴급구호’에 동참하며 1억원을 기부했다. 지민은 국내외 이슈마다 기금을 전하며 소외계층을 지\n",
      "\n",
      "doc=9, num words=240, 창원서 창녕·밀양 접근성 높일 창원 동읍~봉강 도로 10.1㎞ 전 구간 개통 15일 동읍주민센터 앞 용잠교차로 상부서 준공식 개최 2173억 원 투입 2008년 착공해 15년 만에 전체 준공 교통량 분산 주민 통행·주남저수지 방문객 편의 기대 경남 창원 도심에서 주남저수지 일대는 물론 창녕과 밀양으로 가는 접근성 높일 동읍~봉강 도로가 완전히 개통했다. 경\n",
      "\n",
      "doc=10, num words=75, 북한 해킹 조직 ‘김수키’, 방송사·기업도 표적 북한 해커조직 '김수키'가 만든 악성 코드가 방송사와 일반 기업까지 공격 표적으로 삼은 것으로 확인됐습니다. 정보 보안 기업 '안랩'에 따르면 '김수키'가 만든 악성 코드가 자기소개서와 앱 서비스 제안서 등의 형태로 유포되고 있습니다. '안랩' 관계자는 발신자를 알 수 없는 메일의 첨부 파일은 열람을 자제하고\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(Corpus(df_dropna)):\n",
    "    if i <= 5: continue\n",
    "    if i > 10: break\n",
    "    print('doc=%d, num words=%d, %s\\n' % (i, len(doc.split(' ')), doc[:200]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CorpusbasedKeywordExtractor를 만들 때, 애초에 키워드 후보가 될 수 있는 단어를 minimum term frequency (min_tf)와 minimum document frequency (min_df)로 필터링 할 수 있도록 하였습니다. 키워드의 후보들은 모두 min_tf, min_df 이상이 되는 단어들로 한정됩니다.\n",
    "\n",
    "tokenize는 텍스트 형식의 corpus에서 단어를 추출하기 위한 tokenizer입니다. 기본값은 띄어쓰기입니다만, KoNLPy의 nouns()나 pos()를 이용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done 25128 terms, 7898 docs, memory = 0.738 Gb\n"
     ]
    }
   ],
   "source": [
    "from soykeyword.proportion import CorpusbasedKeywordExtractor\n",
    "\n",
    "mecab = Mecab()\n",
    "corpusbased_extractor = CorpusbasedKeywordExtractor(\n",
    "    min_tf=20,\n",
    "    min_df=2,\n",
    "    tokenize=lambda x:mecab.nouns(x),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "corpusbased_extractor.train(Corpus(df_dropna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박근혜 55\n",
      "문재인 192\n",
      "최순실 0\n",
      "아이오아이 0\n",
      "트와이스 0\n",
      "군사 1251\n",
      "외교 2138\n"
     ]
    }
   ],
   "source": [
    "for word in ['박근혜', '문재인', '최순실', '아이오아이', '트와이스', '군사', '외교']:\n",
    "    print(word, corpusbased_extractor.frequency(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 10, 34, 35, 58, 61, 62, 76, 78, 83]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '외교'가 포함된 문서 번호 (텍스트 파일에서의 line number)를 가지고 올 수도 있습니다.\n",
    "documents = corpusbased_extractor.get_document_index('외교')\n",
    "documents[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_from_word(aspect_word)는 기준점이 되는 단어를 넣으면 min_score, min_frequency 이상이 되는 단어들 을 선택합니다. target document는 aspect_word가 포함된 문서 집합이며, reference document는 aspect_word가 포함되지 않은 문서 집합입니다.\n",
    "\n",
    "'외교'가 포함된 문서가 target document이기 때문에 '외교' 단어는 score가 반드시 1.0입니다. 그 외의 키워드에서 '뮌헨', '링컨', '방미', '외무', '조현동', '이란' 등 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KeywordScore(word='외교', frequency=2138, score=1.0),\n",
       " KeywordScore(word='뮌헨', frequency=109, score=0.9870338069501552),\n",
       " KeywordScore(word='링컨', frequency=123, score=0.9844655198967406),\n",
       " KeywordScore(word='방미', frequency=194, score=0.9816726832390705),\n",
       " KeywordScore(word='외무', frequency=142, score=0.9766310794660197),\n",
       " KeywordScore(word='조현동', frequency=156, score=0.9677452793849101),\n",
       " KeywordScore(word='이란', frequency=448, score=0.9664862141387943),\n",
       " KeywordScore(word='한중', frequency=121, score=0.9651446180085678),\n",
       " KeywordScore(word='고위급', frequency=202, score=0.9630989958952829),\n",
       " KeywordScore(word='이사국', frequency=184, score=0.9548784756254747)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = corpusbased_extractor.extract_from_word(\n",
    "    '외교',\n",
    "    min_score=0.7,\n",
    "    min_frequency=100\n",
    ")\n",
    "\n",
    "keywords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect word = 정치 (668)\n",
      "   정치 (668, 1.00)  -      당 (178, 0.92)  -    김주애 (388, 0.92)  -     공직 (177, 0.89)\n",
      "   선거 (307, 0.86)  -     대만 (512, 0.86)  -    인민군 (243, 0.85)  -     건군 (164, 0.85)\n",
      "   행보 (169, 0.84)  -    김정은 (584, 0.84)  -     자제 (162, 0.84)  -     혁명 (172, 0.84)\n",
      "   야당 (182, 0.84)  -     직무 (191, 0.84)  -     국정 (205, 0.83)  -    노동당 (166, 0.83)\n",
      "   전원 (234, 0.83)  -     개입 (165, 0.83)  -    청와대 (233, 0.83)  -     신문 (342, 0.82)\n",
      "   무력 (315, 0.82)  -    김일성 (190, 0.82)  -    의원 (1589, 0.82)  -     갈등 (314, 0.81)\n",
      "   대리 (161, 0.81)  -     소아 (190, 0.81)  -    민주당 (628, 0.81)  -      절 (247, 0.81)\n",
      "   여론 (248, 0.81)  -     통일 (311, 0.80)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Aspect word = 외교 (2138)\n",
      "  외교 (2138, 1.00)  -     방미 (194, 0.98)  -    조현동 (156, 0.97)  -     이란 (448, 0.97)\n",
      "  고위급 (202, 0.96)  -    이사국 (184, 0.95)  -     국빈 (229, 0.95)  -     징용 (614, 0.94)\n",
      "   해법 (544, 0.94)  -     박진 (531, 0.93)  -    안보리 (311, 0.93)  -    워싱턴 (395, 0.93)\n",
      "   비자 (539, 0.93)  -     공조 (328, 0.93)  -     사죄 (210, 0.93)  -     변제 (225, 0.93)\n",
      " 외교부 (2265, 0.92)  -     입국 (410, 0.92)  -     호응 (296, 0.92)  -      日 (189, 0.92)\n",
      "  회담 (1159, 0.92)  -    강제 (1241, 0.92)  -    차관 (1397, 0.92)  -     동맹 (954, 0.91)\n",
      "  양국 (1359, 0.91)  -    국무부 (236, 0.91)  -    태평양 (483, 0.91)  -     미일 (458, 0.91)\n",
      "   현안 (549, 0.91)  -     발급 (459, 0.91)  -     배상 (665, 0.90)  -     포괄 (214, 0.90)\n",
      "  일본 (3130, 0.90)  -    외무성 (179, 0.90)  -    기시다 (217, 0.90)  -    토론회 (437, 0.90)\n",
      "   보복 (192, 0.89)  -     단기 (390, 0.89)  -     발언 (877, 0.89)  -    당국자 (295, 0.89)\n",
      "  비핵화 (228, 0.89)  -    협의 (1490, 0.89)  -     광물 (151, 0.89)  -     순방 (263, 0.88)\n",
      "   동원 (755, 0.88)  -     협상 (189, 0.88)  -     대사 (965, 0.88)  -    안보 (1611, 0.88)\n",
      "--------------------------------------------------------------------------------\n",
      "Aspect word = 국방 (1184)\n",
      "  국방 (1184, 1.00)  -    오스틴 (484, 1.00)  -    로이드 (159, 0.99)  -    이종섭 (283, 0.95)\n",
      "   방산 (449, 0.95)  -     공약 (302, 0.93)  -     방위 (695, 0.93)  -     조달 (150, 0.93)\n",
      "   연습 (509, 0.92)  -    억제 (1124, 0.92)  -     고체 (226, 0.91)  -     미일 (458, 0.90)\n",
      " 국방부 (1675, 0.90)  -      축 (164, 0.90)  -    회담 (1159, 0.90)  -    스텔스 (256, 0.90)\n",
      " 미사일 (1971, 0.90)  -     검열 (188, 0.90)  -     한미 (249, 0.90)  -     화성 (239, 0.89)\n",
      "   전개 (395, 0.89)  -    전투기 (445, 0.89)  -     무인 (153, 0.88)  -     연합 (805, 0.88)\n",
      "   방한 (383, 0.88)  -  스톨텐베르그 (151, 0.87)  -     탄도 (304, 0.87)  -     발사 (746, 0.87)\n",
      "   연료 (273, 0.87)  -     타격 (312, 0.87)  -     운용 (730, 0.86)  -    우크라 (473, 0.86)\n",
      "   신형 (216, 0.86)  -     핵 (2546, 0.86)  -     공중 (342, 0.86)  -     전차 (308, 0.86)\n",
      "  확장 (1494, 0.86)  -    레이더 (313, 0.86)  -     실효 (157, 0.86)  -     공조 (328, 0.86)\n",
      "  군사 (1251, 0.86)  -     실패 (224, 0.86)  -    안보 (1611, 0.86)  -     미 (3593, 0.85)\n",
      "   무력 (315, 0.85)  -    장관 (4105, 0.85)  -     수단 (357, 0.85)  -     식별 (229, 0.85)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for word in ['정치', '외교', '국방']:\n",
    "\n",
    "    keywords = corpusbased_extractor.extract_from_word(\n",
    "        word, min_score=0.8,\n",
    "        min_frequency=150\n",
    "    )\n",
    "\n",
    "    keywords = keywords[:48]\n",
    "\n",
    "    word_frequency = corpusbased_extractor.frequency(word)\n",
    "    print('Aspect word = %s (%d)' % (word, word_frequency))\n",
    "\n",
    "    def in_a_line(subkeywords):\n",
    "        def tuple_to_strf(keyword):\n",
    "            return '%s (%d, %.2f)' % keyword        \n",
    "        strf = [tuple_to_strf(keyword) for keyword in subkeywords]\n",
    "        strf = ['%17s' % s for s in strf]\n",
    "        return '  -  '.join(strf)\n",
    "\n",
    "    for i in range(12):\n",
    "        subkeywords = keywords[4*i:4*i+4]\n",
    "        line = in_a_line(subkeywords)\n",
    "        print(line)\n",
    "\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 머신러닝 알고리즘에 적용하기 위해서 sparse matrix 형식으로 데이터를 저장해둔 경우들도 있습니다. 이때에도 키워드 추출이 용이하도록 MatrixbasedKeywordExtractor를 만들어두었습니다. Interface나 작동 방식은 위와 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7898, 26969)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0.001)\n",
    "x = vectorizer.fit_transform(Corpus(df_dropna))\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn의 CountVectorizer를 이용하여 term frequency matrix; x를 만들고, {word:index}의 dictionary와 [word, ...]의 list of str을 만들어두었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = vectorizer.vocabulary_\n",
    "index2word = sorted(\n",
    "    vectorizer.vocabulary_,\n",
    "    key=lambda x:vectorizer.vocabulary_[x]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix 형식이기 때문에 tokenize는 필요없습니다. 그 외의 parameters는 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixbasedKeywordExtractor trained\n"
     ]
    }
   ],
   "source": [
    "from soykeyword.proportion import MatrixbasedKeywordExtractor\n",
    "\n",
    "matrixbased_extractor = MatrixbasedKeywordExtractor(\n",
    "    min_tf=20,\n",
    "    min_df=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "matrixbased_extractor.train(x, index2word=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KeywordScore(word=25560, frequency=101, score=0.963730934184232),\n",
       " KeywordScore(word=26773, frequency=108, score=0.9576198260296309),\n",
       " KeywordScore(word=6420, frequency=161, score=0.9355011401696003),\n",
       " KeywordScore(word=225, frequency=312, score=0.9322509173994322),\n",
       " KeywordScore(word=25676, frequency=111, score=0.9174517473486796),\n",
       " KeywordScore(word=2699, frequency=112, score=0.9167241129862725),\n",
       " KeywordScore(word=20888, frequency=249, score=0.9136806104549573),\n",
       " KeywordScore(word=19654, frequency=102, score=0.9116628110365063),\n",
       " KeywordScore(word=5167, frequency=105, score=0.909153060234049),\n",
       " KeywordScore(word=15070, frequency=105, score=0.909153060234049)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = matrixbased_extractor.extract_from_word(\n",
    "    5537,\n",
    "    min_score=0.8,\n",
    "    min_frequency=100\n",
    ")\n",
    "\n",
    "keywords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=합의, frequency=101, score=0.964\n",
      "word=효력, frequency=108, score=0.958\n",
      "word=남북, frequency=161, score=0.936\n",
      "word=19, frequency=312, score=0.932\n",
      "word=해당하는, frequency=111, score=0.917\n",
      "word=감시, frequency=112, score=0.917\n",
      "word=제주, frequency=249, score=0.914\n",
      "word=재개, frequency=102, score=0.912\n",
      "word=군사분계선, frequency=105, score=0.909\n",
      "word=아동, frequency=105, score=0.909\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords[:10]:\n",
    "\n",
    "    word = index2word[keyword.word]\n",
    "    frequency = keyword.frequency\n",
    "    score = keyword.score\n",
    "\n",
    "    print('word=%s, frequency=%d, score=%.3f' % (\n",
    "        word, frequency, score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keyword 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docID</th>\n",
       "      <th>date</th>\n",
       "      <th>writerName</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tag</th>\n",
       "      <th>title_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20230214</td>\n",
       "      <td>머니S</td>\n",
       "      <td>정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원</td>\n",
       "      <td>정부가 미국 AI 개발업체인 '오픈AI'(OpenAI)의 '챗GPT'(ChatGPT...</td>\n",
       "      <td>0</td>\n",
       "      <td>정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원 정부가 미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20230215</td>\n",
       "      <td>뉴시스</td>\n",
       "      <td>인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각</td>\n",
       "      <td>기사내용 요약 법원 \"피의 사실 일부 다툼 여지, 불구속 상태 방어권 보장 필요\" ...</td>\n",
       "      <td>0</td>\n",
       "      <td>인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각 기사내용 요약 법원 \"피의 사...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20230214</td>\n",
       "      <td>아이뉴스24</td>\n",
       "      <td>튀르키예 강진에 우리나라 지하수가 출렁였다</td>\n",
       "      <td>튀르키예에서 발생한 강진에 우리나라의 지하수가 출렁였다는 관측 보고가 나왔다. 한국...</td>\n",
       "      <td>0</td>\n",
       "      <td>튀르키예 강진에 우리나라 지하수가 출렁였다 튀르키예에서 발생한 강진에 우리나라의 지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20230215</td>\n",
       "      <td>데일리안</td>\n",
       "      <td>멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행</td>\n",
       "      <td>[데일리안 = 박영민 기자] 오픈마켓 멸치쇼핑이 2023년 신입 및 경력 사원을 대...</td>\n",
       "      <td>0</td>\n",
       "      <td>멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행 [데일리안 = 박영민 기...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20230111</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\"</td>\n",
       "      <td>국방부 대변인 \"실수 바로잡기 위해 내무부와 협력\"…'오류 발견' 가족 등에 연락 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\" 국방부 대변인 \"실수 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7996</td>\n",
       "      <td>20230104</td>\n",
       "      <td>이데일리</td>\n",
       "      <td>외교부, 오는 12일 日강제징용 해법 토론회 연다</td>\n",
       "      <td>국회서 한일의원연맹과 공동 개최 [이데일리 권오석 기자] 외교부가 오는 12일 일제...</td>\n",
       "      <td>1</td>\n",
       "      <td>외교부, 오는 12일 日강제징용 해법 토론회 연다 국회서 한일의원연맹과 공동 개최 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7997</td>\n",
       "      <td>20230215</td>\n",
       "      <td>아시아경제</td>\n",
       "      <td>교원단체, '유치원' 명칭은 일제 잔재…'유아학교'로 변경해야</td>\n",
       "      <td>교육부가 2025년부터 유치원과 어린이집을 하나로 통합하겠다는 계획을 밝힌 가운데 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>교원단체, '유치원' 명칭은 일제 잔재…'유아학교'로 변경해야 교육부가 2025년부...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7998</td>\n",
       "      <td>20230212</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>내일 한일외교차관회담… '강제동원 해법' 이견 좁힐까</td>\n",
       "      <td>13일 워싱턴 한미일 차관협의회 계기 양자회담 예정 日 '사죄·배상' 방식 쟁점… ...</td>\n",
       "      <td>1</td>\n",
       "      <td>내일 한일외교차관회담… '강제동원 해법' 이견 좁힐까 13일 워싱턴 한미일 차관협의...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7999</td>\n",
       "      <td>20230214</td>\n",
       "      <td>매일경제</td>\n",
       "      <td>최상호 국립오페라단 단장</td>\n",
       "      <td>문화체육관광부가 재단법인 국립오페라단 단장 겸 예술감독에 최상호 한국예술종합학교 음...</td>\n",
       "      <td>0</td>\n",
       "      <td>최상호 국립오페라단 단장 문화체육관광부가 재단법인 국립오페라단 단장 겸 예술감독에 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>8000</td>\n",
       "      <td>20230214</td>\n",
       "      <td>스포츠서울</td>\n",
       "      <td>이통통신 3사, 갤럭시 S23 시리즈 사전 개통 시작…'구매 꿀팁은?'</td>\n",
       "      <td>[스포츠서울 | 황철훈기자] 통신 3사가 14일부터 삼성전자 플래그십 스마트폰 ‘갤...</td>\n",
       "      <td>0</td>\n",
       "      <td>이통통신 3사, 갤럭시 S23 시리즈 사전 개통 시작…'구매 꿀팁은?' [스포츠서울...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7898 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      docID      date writerName                                     title  \\\n",
       "0         1  20230214        머니S  정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원   \n",
       "1         2  20230215        뉴시스             인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각   \n",
       "2         3  20230214     아이뉴스24                   튀르키예 강진에 우리나라 지하수가 출렁였다   \n",
       "3         4  20230215       데일리안          멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행   \n",
       "4         5  20230111        뉴스1         美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\"   \n",
       "...     ...       ...        ...                                       ...   \n",
       "7995   7996  20230104       이데일리               외교부, 오는 12일 日강제징용 해법 토론회 연다   \n",
       "7996   7997  20230215      아시아경제        교원단체, '유치원' 명칭은 일제 잔재…'유아학교'로 변경해야   \n",
       "7997   7998  20230212        뉴스1             내일 한일외교차관회담… '강제동원 해법' 이견 좁힐까   \n",
       "7998   7999  20230214       매일경제                             최상호 국립오페라단 단장   \n",
       "7999   8000  20230214      스포츠서울   이통통신 3사, 갤럭시 S23 시리즈 사전 개통 시작…'구매 꿀팁은?'   \n",
       "\n",
       "                                                content  tag  \\\n",
       "0     정부가 미국 AI 개발업체인 '오픈AI'(OpenAI)의 '챗GPT'(ChatGPT...    0   \n",
       "1     기사내용 요약 법원 \"피의 사실 일부 다툼 여지, 불구속 상태 방어권 보장 필요\" ...    0   \n",
       "2     튀르키예에서 발생한 강진에 우리나라의 지하수가 출렁였다는 관측 보고가 나왔다. 한국...    0   \n",
       "3     [데일리안 = 박영민 기자] 오픈마켓 멸치쇼핑이 2023년 신입 및 경력 사원을 대...    0   \n",
       "4     국방부 대변인 \"실수 바로잡기 위해 내무부와 협력\"…'오류 발견' 가족 등에 연락 ...    1   \n",
       "...                                                 ...  ...   \n",
       "7995  국회서 한일의원연맹과 공동 개최 [이데일리 권오석 기자] 외교부가 오는 12일 일제...    1   \n",
       "7996  교육부가 2025년부터 유치원과 어린이집을 하나로 통합하겠다는 계획을 밝힌 가운데 ...    0   \n",
       "7997  13일 워싱턴 한미일 차관협의회 계기 양자회담 예정 日 '사죄·배상' 방식 쟁점… ...    1   \n",
       "7998  문화체육관광부가 재단법인 국립오페라단 단장 겸 예술감독에 최상호 한국예술종합학교 음...    0   \n",
       "7999  [스포츠서울 | 황철훈기자] 통신 3사가 14일부터 삼성전자 플래그십 스마트폰 ‘갤...    0   \n",
       "\n",
       "                                          title_content  \n",
       "0     정부, AI반도체 석·박사 집중 육성… 대학당 '6년간 164억원' 지원 정부가 미...  \n",
       "1     인사 청탁 대가 금품수수 의혹 전 소방청장 영장 기각 기사내용 요약 법원 \"피의 사...  \n",
       "2     튀르키예 강진에 우리나라 지하수가 출렁였다 튀르키예에서 발생한 강진에 우리나라의 지...  \n",
       "3     멸치쇼핑, 2023년 신입 및 경력 사원 대규모 공채 진행 [데일리안 = 박영민 기...  \n",
       "4     美국방부, 추모의 벽 전사자 명단 오류에 \"유감스러운 실수\" 국방부 대변인 \"실수 ...  \n",
       "...                                                 ...  \n",
       "7995  외교부, 오는 12일 日강제징용 해법 토론회 연다 국회서 한일의원연맹과 공동 개최 ...  \n",
       "7996  교원단체, '유치원' 명칭은 일제 잔재…'유아학교'로 변경해야 교육부가 2025년부...  \n",
       "7997  내일 한일외교차관회담… '강제동원 해법' 이견 좁힐까 13일 워싱턴 한미일 차관협의...  \n",
       "7998  최상호 국립오페라단 단장 문화체육관광부가 재단법인 국립오페라단 단장 겸 예술감독에 ...  \n",
       "7999  이통통신 3사, 갤럭시 S23 시리즈 사전 개통 시작…'구매 꿀팁은?' [스포츠서울...  \n",
       "\n",
       "[7898 rows x 7 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done 25128 terms, 7898 docs, memory = 0.619 Gb\n"
     ]
    }
   ],
   "source": [
    "from soykeyword.proportion import CorpusbasedKeywordExtractor\n",
    "\n",
    "macab = Mecab()\n",
    "corpusbased_extractor = CorpusbasedKeywordExtractor(\n",
    "    min_tf=20,\n",
    "    min_df=2,\n",
    "    tokenize=lambda x: mecab.nouns(x),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# docs: list of str like\n",
    "corpusbased_extractor.train(Corpus(df_dropna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " 1000,\n",
       " 1001,\n",
       " 1002,\n",
       " 1003,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1007,\n",
       " 1008,\n",
       " 1009,\n",
       " 1010,\n",
       " 1011,\n",
       " 1012,\n",
       " ...]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = list(df_dropna.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KeywordScore(word='고급', frequency=113, score=0.9935620077255907),\n",
       " KeywordScore(word='대학원', frequency=255, score=0.9927271252151438),\n",
       " KeywordScore(word='신입', frequency=102, score=0.9883455262975699),\n",
       " KeywordScore(word='주권', frequency=104, score=0.9881174869061627),\n",
       " KeywordScore(word='소프트웨어', frequency=335, score=0.9872443098864283),\n",
       " KeywordScore(word='설계', frequency=428, score=0.9837293170907253),\n",
       " KeywordScore(word='반도체', frequency=1226, score=0.9825350303127635),\n",
       " KeywordScore(word='공고', frequency=164, score=0.9813249008959465),\n",
       " KeywordScore(word='실전', frequency=165, score=0.9812124822435045),\n",
       " KeywordScore(word='창출', frequency=189, score=0.9785221404743407)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = corpusbased_extractor.extract_from_docs(\n",
    "    [0],\n",
    "    min_score=0,\n",
    "    min_frequency=100\n",
    ")\n",
    "keywords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7898/7898 [01:26<00:00, 91.81it/s]\n"
     ]
    }
   ],
   "source": [
    "keywords_list = []\n",
    "for i in tqdm(documents):\n",
    "    keywordscore = corpusbased_extractor.extract_from_docs(\n",
    "        [i],\n",
    "        min_score=0.7,\n",
    "        min_frequency=100\n",
    "    )\n",
    "    keywords_nbr = min(10, len(keywordscore))\n",
    "    keywords = [(x.word, x.score) for x in keywordscore if len(x.word)>=2][:keywords_nbr]\n",
    "    keywords_list.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaiv\\AppData\\Local\\Temp\\ipykernel_7660\\1657053099.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_dropna['keywords'] = keywords_list\n"
     ]
    }
   ],
   "source": [
    "df_dropna['keywords'] = keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in keywords_list if len(x)==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(df, df_dropna[['docID', 'keywords']], on='docID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('../data/train_result.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
